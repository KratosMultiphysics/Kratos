extern "C" %{
/**
 *
 * @file zgetrf_sp1dplus.jdf
 *
 * PaRSEC 1D algorithm jdf for LU factorization.
 *
 * @copyright 2016-2018 Bordeaux INP, CNRS (LaBRI UMR 5800), Inria,
 *                      Univ. Bordeaux. All rights reserved.
 *
 * @version 6.0.1
 * @author Mathieu Faverge
 * @date 2018-07-16
 * @precisions normal z -> s d c
 *
 **/
#include <parsec.h>
#include <parsec/data_distribution.h>
#include <parsec/private_mempool.h>
#include "common.h"
#include "solver.h"
#include "sopalin_data.h"
#include "pastix_zcores.h"
#include "pastix_zcuda.h"

%}

/* Globals
 */
descA        [type = "parsec_sparse_matrix_desc_t *" ]
sopalin_data [type = "sopalin_data_t *" ]

forced_pushout [type = "int" hidden = on default = "(0)" ]
datacode  [type = "SolverMatrix*"         hidden = on default = "(sopalin_data->solvmtx)"       ]
cblknbr   [type = "pastix_int_t"          hidden = on default = "(datacode->cblknbr - 1)"       ]
bloknbr   [type = "pastix_int_t"          hidden = on default = "(datacode->bloknbr - 2)"       ]
lowrank   [type = "pastix_lr_t"           hidden = on default = "(sopalin_data->solvmtx->lowrank)"]

p_work    [type = "parsec_memory_pool_t *"]
lwork     [type = "pastix_int_t"]

cpu_coefs [ type = "pastix_fixdbl_t *" hidden = on default = "&((*(sopalin_data->cpu_coefs))[PastixKernelGEMMCblk2d2d][0])" ]
gpu_coefs [ type = "pastix_fixdbl_t *" hidden = on default = "&((*(sopalin_data->gpu_coefs))[PastixKernelGEMMCblk2d2d][0])" ]

/**************************************************
 *                   GETRF                        *
 * panel factorization: do trf of diagonal and    *
 *                    : trsm on off-diagonal      *
 **************************************************/
GETRF(k) [high_priority = on]

// Execution space
k = 0 .. cblknbr

browk0    = %{ SolverCblk *cblk = datacode->cblktab + k;     return cblk->brownum; %}
browk1    = %{ SolverCblk *cblk = datacode->cblktab + k + 1; return cblk->brownum; %}
lastbrow  = %{ if ( browk0 == browk1 ) return 0; else return datacode->browtab[ browk1 - 1 ]; %}
firstblok = %{ SolverCblk *cblk = datacode->cblktab + k;     return cblk->fblokptr - datacode->bloktab + 1; %}
lastblok  = %{ SolverCblk *cblk = datacode->cblktab + k + 1; return cblk->fblokptr - datacode->bloktab - 1; %}

// Parallel partitioning
:descA(0, k, 0)

// Parameters
/* C is A(k) if it's a leaf or get the cblk from the last update */
RW L <- ( browk0 == browk1 ) ? descA(0, k, 0) : C GEMM1D( 0, lastbrow )
     -> A GEMM1D(0 .. 1, firstblok .. lastblok)
     -> descA(0, k, 0)

RW U <- ( browk0 == browk1 ) ? descA(1, k, 0) : C GEMM1D( 1, lastbrow )
     -> B GEMM1D(0 .. 1, firstblok .. lastblok)
     -> descA(1, k, 0)

; %{ return cblknbr - k; %}

BODY
{
    SolverCblk *cblk = datacode->cblktab + k;
    if (!(cblk->cblktype & CBLK_IN_SCHUR)) {
        cpucblk_zgetrfsp1d_panel( datacode, cblk, L, U );
    }
}
END

/**
 *       GEMM
 *
 * To have a contiguous range of GEMM to release in the potrf, they are numbered
 * with the indexes of the off-diagonal blocks, diagonal block included.
 * Thus, the diagonal block tasks which doesn't perfom computations are used as
 * DATA_IN tasks. This is mandatory when using the GPU, due to the versioning
 * bumped by the cpu version of the diagonal block that coccurs when computing
 * the diagonal blocks.
 *
 * For all off-diagonal blocks, it updates the trailing matrix with the panel
 * k-th block updating corresponding.
 *
 */
GEMM1D(s, bloknum)

// Execution space
s       = 0 .. 1
bloknum = 1 .. bloknbr

lcblknm = %{ SolverBlok *blok = datacode->bloktab + bloknum;     return blok->lcblknm; %}
fcblknm = %{ SolverBlok *blok = datacode->bloktab + bloknum;     return blok->fcblknm; %}
first   = %{ SolverCblk *cblk = datacode->cblktab + fcblknm;     return cblk->brownum; %}
last    = %{ SolverCblk *cblk = datacode->cblktab + fcblknm + 1; return cblk->brownum - 1; %}

brownum = %{ return datacode->bloktab[bloknum].browind; %} /* -1 if diagonal block */
prev    = %{
    assert( first >= 0 );
    /**
     * If bloknum is a diagonal block, or if it is the first one applied on C,
     * there is no previous
     */
    if ((brownum == -1) || (brownum == first) ) {
        return 0;
    }
    /**
     * Otherwise we return the previous block in the list of blocks facing fcblk
     */
    else {
        assert( brownum > first );
        return datacode->browtab[brownum-1];
    }
    %}
next    = %{
    /**
     * If we are on a diagonal blok, or if we are the last one, there is no next
     */
    if ((brownum == -1) || (brownum >= last) ){
        return 0;
    } else {
        return datacode->browtab[brownum+1];
    }
    %}

n  = %{ if (brownum == -1) return 0; else return blok_rownbr(datacode->bloktab + bloknum); %}
k  = %{ if (brownum == -1) return 0; else return cblk_colnbr(datacode->cblktab + lcblknm); %}
m  = %{ if (brownum == -1) return 0; else {
        if ((datacode->cblktab + lcblknm)->cblktype & CBLK_LAYOUT_2D) {
            return datacode->cblktab[lcblknm].stride - (datacode->bloktab[bloknum].coefind / k);
        } else {
            return datacode->cblktab[lcblknm].stride - datacode->bloktab[bloknum].coefind;
        }
    }%}

// Parallel partitioning
:descA(s, fcblknm, 0)

// Parameters
READ  A  <- (brownum != -1) ? L GETRF( lcblknm ) : NULL
READ  B  <- (brownum != -1) ? U GETRF( lcblknm ) : NULL

RW    C  <- (brownum == -1) ? NULL
         <- (brownum != -1) && (brownum == first) ? descA( s, fcblknm, 0 )
         <- (brownum != -1) && (brownum != first) ? C GEMM1D( s, prev )

         -> (brownum != -1) && (brownum == last) && (s == 0) ? L GETRF( fcblknm )
         -> (brownum != -1) && (brownum == last) && (s != 0) ? U GETRF( fcblknm )
         -> (brownum != -1) && (brownum != last)             ? C GEMM1D( s, next )

; %{ return cblknbr - ((fcblknm + lcblknm) / 2 ) + last - brownum; %}

BODY [ type=CUDA
       pushout=forced_pushout
       device=%{ SolverCblk *lcblk = datacode->cblktab + lcblknm;
                 SolverCblk *fcblk = datacode->cblktab + fcblknm;
                 if ( (brownum == -1) ||
                      (lcblk->cblktype & (CBLK_COMPRESSED | CBLK_IN_SCHUR)) ||
                      (fcblk->cblktype & CBLK_COMPRESSED) )
                 {
                     return -2;
                 } else {
                     return -1;
                 }
                 %}
       weight="(last-brownum+1)"
       cpu_cost="modelsGetCost3Param( cpu_coefs, m, n, k )"
       gpu_cost="modelsGetCost3Param( gpu_coefs, m, n, k )" ]
#if defined(PASTIX_WITH_CUDA)
{
    /* Never execute the GPU kernel on diagonal blocks */
    if (brownum != -1) {
        SolverCblk *lcblk = datacode->cblktab + lcblknm;
        SolverCblk *fcblk = datacode->cblktab + fcblknm;
        SolverBlok *blok  = datacode->bloktab + bloknum;

        assert( !(lcblk->cblktype & CBLK_COMPRESSED) &&
                !(fcblk->cblktype & CBLK_COMPRESSED) );

        if (!(lcblk->cblktype & CBLK_IN_SCHUR)) {
            if ( blok+s < lcblk[1].fblokptr ) {
                if ( s == 0 ) {
#if defined(PASTIX_CUDA_FERMI)
                    gpu_zgemmsp_fermi( datacode,
                                       PastixLower, PastixTrans,
                                       descA->d_blocktab[parsec_body.index],
                                       lcblk, blok, fcblk,
                                       A, B, C,
                                       parsec_body.stream );
#else
                    gpucblk_zgemmsp( PastixLCoef, PastixUCoef, PastixTrans,
                                     lcblk, blok, fcblk,
                                     A, B, C,
                                     &lowrank, parsec_body.stream );
#endif /* defined(PASTIX_CUDA_FERMI) */
                }
                else {
#if defined(PASTIX_CUDA_FERMI)
                    gpu_zgemmsp_fermi( datacode,
                                       PastixUpper, PastixTrans,
                                       descA->d_blocktab[parsec_body.index],
                                       lcblk, blok, fcblk,
                                       B, A, C,
                                       parsec_body.stream );
#else
                    gpucblk_zgemmsp( PastixUCoef, PastixLCoef, PastixTrans,
                                     lcblk, blok, fcblk,
                                     B, A, C,
                                     &lowrank, parsec_body.stream );
#endif /* defined(PASTIX_CUDA_FERMI) */
                }
            }
        }
    }
}
#endif
END

BODY
{
    /* If diagonal block, we skip it */
    if (brownum != -1) {
        SolverCblk *lcblk = datacode->cblktab + lcblknm;
        SolverCblk *fcblk = datacode->cblktab + fcblknm;
        SolverBlok *blok  = datacode->bloktab + bloknum;
        pastix_complex64_t *work = NULL;

        if (!(lcblk->cblktype & CBLK_IN_SCHUR)) {
            /* Update */
            if ( blok+s < lcblk[1].fblokptr ) {
                if ( lwork > 0 ) {
                    work = (pastix_complex64_t *)parsec_private_memory_pop( p_work );
                }

                if (s == 0) {
                    cpucblk_zgemmsp( PastixLCoef, PastixUCoef, PastixTrans,
                                     lcblk, blok, fcblk,
                                     A, B, C,
                                     work, lwork, &lowrank );
                }
                else {
                    cpucblk_zgemmsp( PastixUCoef, PastixLCoef, PastixTrans,
                                     lcblk, blok, fcblk,
                                     B, A, C,
                                     work, lwork, &lowrank );
                }

                if ( work  ) {
                    parsec_private_memory_push( p_work, (void *)work );
                }
            }
        }
    }
}
END
